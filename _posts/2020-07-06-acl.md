---
layout: post
mathjax: true
title: "ACL 2020"
tags:
- Dialog
categories:
- Applied
thumbnail_path: blog/personal/acl-logo.png
---

### Recursive Template-based Frame Generation for Task Oriented Dialog

- NLU Systems - Intent and Slots (ATIS example).
- ATIS has a shallow hierarchy (from_loc.city_name) but usually these are completely ignored but this limits sharing of data and statistical strength across labels.
- In restaurant domain, we can apply hierarchy to slots - orderitems.[item].{quantity, name, size}
- Contributions
	- Recursive Hierarchical Frame Based Representation that captures complex relationships between slot labels and intents.
	- Formulate Frame Generation as a template-based tree-decoding task (use Pointer Mechanism to copy slot values from input utterance).
	- Tree-based loss function with global supervision and optimize jointly for all loss functions end-to-end.
- Approach
	- Encoder - this paper used BERT as encoder but can be any.
	- Slot Decoder - predicts general form of slot (city)name, month_name) from each step.
	- Template-based Tree Decoder - decode hierarchical representation of slots (introduces NT (non-terminal) concept).
	- Pointer Network - to predict positions for every terminal pointing to a specific token in the user sentence.
	- Global Context - Tree decoder tends to repeat nodes since representations may remain similar from parent to child and we overcome this by providing global supervision.
- Evaluation
	- ATIS and Simulated Restaurant Ordering Dataset
	- EM and F1

### Unknown Intent Detection using Gaussian Mixture Model with an application to Zero-shot Intent Classification

- Problem Formulation
	- Given a training set of seen classes, can we train a model to predict {seen classes, unknown}.
	- Can we take it a step further by predicting the unknown class.
- Gaussian Mixture Loss
	- Better than softmax loss
	- Large-margin Gaussian Mixture Cross-entropy loss (margin is adaptive and is the distance between utterance and class feature centroid)
- Semantic enhancement via class description
	- we extract features from description of class (1 word or a sentence)
	- We assign this to be the class centroid.
- Identify unknown intent for generalized zero-shot learning
	- Local outlier Factor (LOF) - unsupervised density-based anomaly detection method.
	- The intuition is that objects that have a substantially lower relative density value than their neighbors are considered to be outliers.

### Multi-Domain Dialog Acts and Response Co-Generation

- DST + NLG
- Hierarchical Dialog Acts (domains, actions, slots)
- Previously represent dialog acts as a one-hot vector.
- Convert dialog act to a generation problem and the representation is from the sequence model.
- Act Generator and Response Generator share same encoder and input.
- Adopt uncertainty loss (Kendall et al 2018)


### A Contextual Hierarchical Attention Network with Adaptive Objective for DST

- Slot Imbalance
	- Focal loss instead of Cross entropy loss?
	- Adaptive objective
- CHAN Approach
	- BERT for slots and values (treat as sentence and is fixed)
	- Concat user and system utterance for every turn and calculate attention (slot-word attention). This is trainable.
	- This is concatenated to form context encoder which is a transformer encoder network.
	- Slot0turn attention - also multihead to calculate attention across turns.
- State Transition Prediction
	- Predict v_t == v_t-1 for every slot.

### XiaIce

- Introduction
	- Designed to be an AI companion
	- Based on empathetic computing, integrating IQ and EQ
	- Optimized for expected conversation-turns per session (CPS)
	- aimed to pass time-sharing test
- Design Principle
	- IQ: set of skills to keep up with the user and complete tasks
	- EQ: empathy and social skills
	- Personality: consistent
	- Social Chat
		- Hierarchical decision making
		- Top-level (select dialog skills) and Low level (choose primitive actions).
		- Optimize CPS
- Architecture and Core Conversation Engine
	- Hybrid system
	- 3 layers - UX layer, Conversation Engine layer, Data Layer
	- Conversation Engine
		- Dialog Manager
		- Empathetic computing
		- Core Chat
		- Skills
			- 230 skills released since 2014
	- Core Chat
		- General chat skill and domain chat skills
		- Candidate Generation and Ranking
- Ethics Concerns
	- Privacy
	- Control
	- Set right expectation (XiaIce is a bot)


### A Generative Model for Joint NLU & G

- NLU - takes natural language to semantic representation
- NLG - is the reverse of NLU
- JUG
	- latent variable z
	- NLU is p(y \| z, x)
	- NLG is p(x \| z, y)
- Objectives
	- optimize joint probability of x and y.


### What does BERT with Vision Look At? (VisualBERT)

- VisualBERT
	- Image regions and language are combined with a Transformer to allow self-attention to discover implicit alignments between language and vision.
	- Pretrained with 2 tasks - MLM and sentence-image prediction task
- What does BERT with Vision learn during pre-training?
	- Entity Grounding
		- Some attention heads map entity to image regions
		- Accuracy peaks in higher layers (10 and 11)
		- Model deepens its understanding of image as layers go.
	- Syntactic Grounding
		- Non-entity words attend to image regions (for example "wearing" attends to "man region")
		- Different attention heads specialize in different relations.


### Image Chat: Engaging Grounded Conversations

- The goal is to have machines engage with humans in conversations.
- Communication grounded in images is one of the ways to achieve this.
- Given (image, style trait) -> write the message in the conversation
- Models
	- Retrieval-based and Generative Dialog Models
		- Image Encoder
			- ResNet152
			- ResNeXt-IG-3.5B
		- Style Encoder
			- Embed style into N-dimensional vector
		- Dialog History Encoder
			- Transformer-based encoder
			- Reddit dataset (next utterance)
	- Retrieval model (TransResNet-RET)
		- Multimodal combiner (sum or attention)
		- Final Score is dot product of combined representation with Candidate Representation
		- Training has 499 negative candidates.
	- Generative model (TransResNet-GEN)
		- Dialog Encoder
			- Jointly encodes the style and dialog history
		- Dialog Decoder
			- CONCATE(image_encoding, dialog encoder)
			- standard Seq2Seq Transformer
			- Beam search with beam size 2 and trigram blocking at inference time.

### Adversarial NLI: A New Benchmark for NLU

- Are current NLU models as good as their high performance on standard benchmark?
- They are vulnerable to adversaries.
- Them model is brittle and general NLU is far from achieved despite SoTA.
- HAMLET (Human and Model in the Loop Enabled Training)
	- Write examples
	- Get model feedback
	- Verify examples and make splits
	- Retrain
- Adversarial NLI
	- ANLI is smaller than SNLI and MNLI but more useful and robust
	- Error rates decreases as we progress through rounds (A1, A2 etc)
	- Model error rates halved with just 3 rounds


### BART

- See [this post](https://www.weak-learner.com/blog/2020/06/13/BART/)

### Enabling Language Models To Fill in the Blanks

- Editing and Revising
	- We often write in a non-linear manner.
	- Existing auto-complete system only considers the preceding text.
- Connecting Ideas
	- Writing novel and connecting ideas.
- This is **Text Infilling**
	- Arbitrary number of blanks
	- Each blank has arbitrary number of words
- Enable LMs to perform task of infilling
	- GPT2 only left to right
	- BERT must know exact number of tokens
- ILM (Infilling by Language Modeling)
- Works better than GPT2.

### BLEURT: Learning Robust Metrics for Text Generation

- NLG (Translation, Abstractive Summarization and Data-Document Generation)
- How do we evaluate NLG?
	- Human evaluation :(
	- Automatic Metric?
- Automatic metric?
	- Given (candidate sentence, reference sentence) - can we come up with a score?
	- BLEU (NGram overlap)
		- no paraphrasing
		- no synonyms
- WMT Metrics task
- Hybrid Metrics
	- BERTScore (BERT representation dot product)
	- more robust
- E2E Metrics
	- Take pair of sentences as input and produce rating
	- BEER, RUSE
	- more flexible
- BLEURT
	- transformer model with regression objective
	- 4 steps (2 pre-training and 2 fine-tuning)
	- Pre-training: BERT and synthetic sentence pairs
		- Random substitutions
		- Back-translation
		- Random Deletions
		- 15 Existing Metrics (ROUGE, BERTSCore, Entailment, Sentence BLEU)
- Open sourced (GitHub)

### Dialogue Dodecathlon: Open Domain Knowledge and Image Grounded Conversational Agents

- Introduction
	- Goal
		- Conv Agent that can have multiple skills (empathy, knowledge, personable and engaging).
		- Should work in multiple modalities
		- multitask
- DodecaDialog
	- 12 sub-tasks
	- Get to know you when you first talk to it (ConvAI2)
	- Discuss everyday topics (DailyDialog, pushshift.io, Twitter)
	- Speak knowledgeably at depth (Wizard of Wiki, Ubuntu)
	- Answer questions on such topics (ELI5)
	- Demonstate empathy (Empathetic Dialog, LIGHT)
	- Discuss Images (Image Chat, IGC)
- Models
	- Generative BERT Baseline
		- only fine-tuned for text based tasks
	- Image + Seq2Seq
		- ResNeXt Pretrained on 3.5B IG Images

### Reverse Engineering Configurations of Neural Text Generation Models

- Models leave detectable artifacts
	- Do some modeling choices leave behind more artifacts than others?
	- Can we distinguish between text generation models based on text generated alone?
	- Which model configurations leave behind the most detectable artifacts?
- Given generated text, we try to predict the configurations.
- Configurations
	- Top-K and Top-p nucleus sampling
	- Length of initial conditional text
	- Model size (base, large, mega)


### Span-ConverRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations

- Summary
	- light-weight model for dialog slot-filling
	- also present a unique dataset
- Multi-Woz has categorical slots and ATIS has single turn interactions
- Restaurants8k
	- GitHub available
	- 5 slots (time, date, people, first, last name)
- Models
	- Subword vectors + CNN + CRF
	- Read about ConverRT


### Coach: A Coarse-to-Fine Approach for Cross-domain Slot Filling

- Background
	- cross-domain slot filling
	- 0 or few labels in target domain
- Concept Tagger (Bapna et al 2017)
	- Conducts slot filling for each slot type 
- Coach Framework
	- Crossly learn the pattern of entities (only 3 tags (B, I, O))
	- In second step, we predict the specific slot step (slot description and similarity matrix)
- Template Regularization
	- create templates and fill with wrong slots
- SNIPS and CoNLL 2003


### MuTual: A dataset for Multi-Turn dialog reasoning

- Reasoning in chit-chat dialog
	- chatbots tend to select the best surface matching candidates
- Listening Comprehension
	- High school listening comprehension
	- Choose best answer from 3 options


### Conversational Word Embedding for Retrieval-Based Dialog System

- Word representation
	- Static
	- Contextual
- Previous word embedding methods for conversation
	- single sentence
	- single vector space (both post and reply are in same vector space)
- PR-Embedding
	- from conversation pairs in 2 vector spaces (post and reply)
	- 2 embedding matrix
- Sentence level learning
	- Dot Product, CNN and Max-pool

### Multimodal Transformer for Multimodal Machine Translation

- Goal
	- Input is (Image, Source language text)
	- Output is target language text
- Problem
	- Image is not as important
	- Text is more important
	- We should consider relative importance between them
- Methodology
	- Extract features from image and fed into transformer.
	- Perform multimodal self attention
		- only query contains multimodal
		- key and value only contain text
		- see image for details


### Slot Consistent NLG for Task-oriented Dialog Systems with Iterative Rectification Network

- Motivation
	- NLG (convert meaningful representation to natural language)
	- use Dialog Acts
	- Convert NLG to Template to Lexicalization to Natural Language
	- Delexicalization
		- replace all slot values by their corresponding slot type in DA
	- Hallucination
		- misplacement error of unseen slots
		- missing slots
- Approach
	- Slot Extraction function and Slot consistent objective
		- g(f(x)) = g(x)
	- Model
		- Iterative process
		- Generate word/copy of template elements
		- Obtain training samples from a buffer that consists of mistaken NLG results


### Stanza - A Python NLP Toolkit for Many Human Languages

- Performance of toolkits compromised for multiple languages
	- very limited multilingual linguistic resources
	- Models (traditional toolkits have hand-written rules)
- Stanza
	- Full support for Universal dependencies v2
	- Fully neural pipeline
	- currently support 66 human languages
	- Python client interface to CoreNLP library
- Stanza Neural Components
	- Tokenization / Sentence Segmentation
		- character sequence tagging (split at current position?)
	- Multi-word token expansion
		- expands to syntactic words
		- Seq2Seq (Dictionary as L1)
	- POS Tagger
		- same as dependency parser below
	- Lemmatization
		- same as Multi-word token expansion
	- Dependency Parsing
		- Head and dependent representation for each word
		- Bi-affine Transform
	- NER
		- Neural BiLSTM-CRF tagger enhanced with character LMs (FLAIR, Akbik et al)
- Evaluation
	- Better than SpaCy in accuracy and equal to FLAIR
	- slower than SpaCy (10x slower!!)


### Photon: A Robust Cross-Domain Text-to-SQL System

- Live Demo at http://naturalsql.com/
- Natural Language to SQL
	- accurately map NL input to executable SQL queries
	- work across different databases
	- robustness (don't know is better than mistakes)
	- support user interaction
- Photon
	- SotA neural text to SQL
	- Novel Confusion detection approach
	- Template based response generation for user interaction
- Text to SQL Semantic Parsing
	- Spider dataset
	- Seq2Seq Encoder-Decoder
	- Encoder (NL + Schema)
	- Pointer Generator Decoder + Cross entropy loss
- Confusion Detection
	- UTran-SQL dataset
	- back-translation and adverserial filtering
	- Binary classification and confusion span detector


### Efficient DST by selectively overwriting memory

- DST is defined by a set of (slot, value) pairs.
- Pre-defined ontology based vs open-vocab based.
- This paper is about the later.
- MultiWoz dataset (2.1)
- Selectively overwriting memory
	- Carryover or update from previous turn?
- State Operation
	- Carryover, delete, don't care or update
- DST is divided into 2 tasks - 
	- state operation and slot generation


### Improving Low-Resource NER using Joint Sentence and Token Labeling

- Low-Resource
	- Thai, Vietnamese and Indonesian
	- Domain - e-commerce
	- 2k annotation per language and this leads to overfitting
	- BiLSTM CRF
- Improving
	- Introduce fake task of predicting category
	- Maxpooling on final hidden represenation and feed through linear layer to predict category.
	- This helps in overfitting and helps in generalization.
- Attention
	- squeeze performance by a little.


### Soft Gazetteers for Low-Resource NER

- Integrating handcrafter features with neural models is useful for NERs
- Gazetteer features
	- very limited for low-resource languages
- Soft Gazetteer
	- English KBs only
	- can use entity linking
	- Features
		- top-3 candidate scores
		- top-3 type-wise counts
		- top-30 type-wise counts
		- margins between top-4 candidates
- BiLSTM CRF
