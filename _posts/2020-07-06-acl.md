---
layout: post
mathjax: true
title: "ACL 2020"
tags:
- Dialog
categories:
- Applied
thumbnail_path: blog/personal/acl-logo.png
---

### Recursive Template-based Frame Generation for Task Oriented Dialog

- NLU Systems - Intent and Slots (ATIS example).
- ATIS has a shallow hierarchy (from_loc.city_name) but usually these are completely ignored but this limits sharing of data and statistical strength across labels.
- In restaurant domain, we can apply hierarchy to slots - orderitems.[item].{quantity, name, size}
- Contributions
	- Recursive Hierarchical Frame Based Representation that captures complex relationships between slot labels and intents.
	- Formulate Frame Generation as a template-based tree-decoding task (use Pointer Mechanism to copy slot values from input utterance).
	- Tree-based loss function with global supervision and optimize jointly for all loss functions end-to-end.
- Approach
	- Encoder - this paper used BERT as encoder but can be any.
	- Slot Decoder - predicts general form of slot (city)name, month_name) from each step.
	- Template-based Tree Decoder - decode hierarchical representation of slots (introduces NT (non-terminal) concept).
	- Pointer Network - to predict positions for every terminal pointing to a specific token in the user sentence.
	- Global Context - Tree decoder tends to repeat nodes since representations may remain similar from parent to child and we overcome this by providing global supervision.
- Evaluation
	- ATIS and Simulated Restaurant Ordering Dataset
	- EM and F1

### Unknown Intent Detection using Gaussian Mixture Model with an application to Zero-shot Intent Classification

- Problem Formulation
	- Given a training set of seen classes, can we train a model to predict {seen classes, unknown}.
	- Can we take it a step further by predicting the unknown class.
- Gaussian Mixture Loss
	- Better than softmax loss
	- Large-margin Gaussian Mixture Cross-entropy loss (margin is adaptive and is the distance between utterance and class feature centroid)
- Semantic enhancement via class description
	- we extract features from description of class (1 word or a sentence)
	- We assign this to be the class centroid.
- Identify unknown intent for generalized zero-shot learning
	- Local outlier Factor (LOF) - unsupervised density-based anomaly detection method.
	- The intuition is that objects that have a substantially lower relative density value than their neighbors are considered to be outliers.

### Multi-Domain Dialog Acts and Response Co-Generation

- DST + NLG
- Hierarchical Dialog Acts (domains, actions, slots)
- Previously represent dialog acts as a one-hot vector.
- Convert dialog act to a generation problem and the representation is from the sequence model.
- Act Generator and Response Generator share same encoder and input.
- Adopt uncertainty loss (Kendall et al 2018)


### A Contextual Hierarchical Attention Network with Adaptive Objective for DST

- Slot Imbalance
	- Focal loss instead of Cross entropy loss?
	- Adaptive objective
- CHAN Approach
	- BERT for slots and values (treat as sentence and is fixed)
	- Concat user and system utterance for every turn and calculate attention (slot-word attention). This is trainable.
	- This is concatenated to form context encoder which is a transformer encoder network.
	- Slot0turn attention - also multihead to calculate attention across turns.
- State Transition Prediction
	- Predict v_t == v_t-1 for every slot.

### XiaIce

- Introduction
	- Designed to be an AI companion
	- Based on empathetic computing, integrating IQ and EQ
	- Optimized for expected conversation-turns per session (CPS)
	- aimed to pass time-sharing test
- Design Principle
	- IQ: set of skills to keep up with the user and complete tasks
	- EQ: empathy and social skills
	- Personality: consistent
	- Social Chat
		- Hierarchical decision making
		- Top-level (select dialog skills) and Low level (choose primitive actions).
		- Optimize CPS
- Architecture and Core Conversation Engine
	- Hybrid system
	- 3 layers - UX layer, Conversation Engine layer, Data Layer
	- Conversation Engine
		- Dialog Manager
		- Empathetic computing
		- Core Chat
		- Skills
			- 230 skills released since 2014
	- Core Chat
		- General chat skill and domain chat skills
		- Candidate Generation and Ranking
- Ethics Concerns
	- Privacy
	- Control
	- Set right expectation (XiaIce is a bot)


### A Generative Model for Joint NLU & G

- NLU - takes natural language to semantic representation
- NLG - is the reverse of NLU
- JUG
	- latent variable z
	- NLU is p(y \| z, x)
	- NLG is p(x \| z, y)
- Objectives
	- optimize joint probability of x and y.


### What does BERT with Vision Look At? (VisualBERT)

- VisualBERT
	- Image regions and language are combined with a Transformer to allow self-attention to discover implicit alignments between language and vision.
	- Pretrained with 2 tasks - MLM and sentence-image prediction task
- What does BERT with Vision learn during pre-training?
	- Entity Grounding
		- Some attention heads map entity to image regions
		- Accuracy peaks in higher layers (10 and 11)
		- Model deepens its understanding of image as layers go.
	- Syntactic Grounding
		- Non-entity words attend to image regions (for example "wearing" attends to "man region")
		- Different attention heads specialize in different relations.


### Image Chat: Engaging Grounded Conversations

- The goal is to have machines engage with humans in conversations.
- Communication grounded in images is one of the ways to achieve this.
- Given (image, style trait) -> write the message in the conversation
- Models
	- Retrieval-based and Generative Dialog Models
		- Image Encoder
			- ResNet152
			- ResNeXt-IG-3.5B
		- Style Encoder
			- Embed style into N-dimensional vector
		- Dialog History Encoder
			- Transformer-based encoder
			- Reddit dataset (next utterance)
	- Retrieval model (TransResNet-RET)
		- Multimodal combiner (sum or attention)
		- Final Score is dot product of combined representation with Candidate Representation
		- Training has 499 negative candidates.
	- Generative model (TransResNet-GEN)
		- Dialog Encoder
			- Jointly encodes the style and dialog history
		- Dialog Decoder
			- CONCATE(image_encoding, dialog encoder)
			- standard Seq2Seq Transformer
			- Beam search with beam size 2 and trigram blocking at inference time.

### Adversarial NLI: A New Benchmark for NLU

- Are current NLU models as good as their high performance on standard benchmark?
- They are vulnerable to adversaries.
- Them model is brittle and general NLU is far from achieved despite SoTA.
- HAMLET (Human and Model in the Loop Enabled Training)
	- Write examples
	- Get model feedback
	- Verify examples and make splits
	- Retrain
- Adversarial NLI
	- ANLI is smaller than SNLI and MNLI but more useful and robust
	- Error rates decreases as we progress through rounds (A1, A2 etc)
	- Model error rates halved with just 3 rounds


### BART

- See [this post](https://www.weak-learner.com/blog/2020/06/13/BART/)

### Enabling Language Models To Fill in the Blanks

- Editing and Revising
	- We often write in a non-linear manner.
	- Existing auto-complete system only considers the preceding text.
- Connecting Ideas
	- Writing novel and connecting ideas.
- This is **Text Infilling**
	- Arbitrary number of blanks
	- Each blank has arbitrary number of words
- Enable LMs to perform task of infilling
	- GPT2 only left to right
	- BERT must know exact number of tokens
- ILM (Infilling by Language Modeling)
- Works better than GPT2.

### BLEURT: Learning Robust Metrics for Text Generation

- NLG (Translation, Abstractive Summarization and Data-Document Generation)
- How do we evaluate NLG?
	- Human evaluation :(
	- Automatic Metric?
- Automatic metric?
	- Given (candidate sentence, reference sentence) - can we come up with a score?
	- BLEU (NGram overlap)
		- no paraphrasing
		- no synonyms
- WMT Metrics task
- Hybrid Metrics
	- BERTScore (BERT representation dot product)
	- more robust
- E2E Metrics
	- Take pair of sentences as input and produce rating
	- BEER, RUSE
	- more flexible
- BLEURT
	- transformer model with regression objective
	- 4 steps (2 pre-training and 2 fine-tuning)
	- Pre-training: BERT and synthetic sentence pairs
		- Random substitutions
		- Back-translation
		- Random Deletions
		- 15 Existing Metrics (ROUGE, BERTSCore, Entailment, Sentence BLEU)
- Open sourced (GitHub)

### Dialogue Dodecathlon: Open Domain Knowledge and Image Grounded Conversational Agents

- Introduction
	- Goal
		- Conv Agent that can have multiple skills (empathy, knowledge, personable and engaging).
		- Should work in multiple modalities
		- multitask
- DodecaDialog
	- 12 sub-tasks
	- Get to know you when you first talk to it (ConvAI2)
	- Discuss everyday topics (DailyDialog, pushshift.io, Twitter)
	- Speak knowledgeably at depth (Wizard of Wiki, Ubuntu)
	- Answer questions on such topics (ELI5)
	- Demonstate empathy (Empathetic Dialog, LIGHT)
	- Discuss Images (Image Chat, IGC)
- Models
	- Generative BERT Baseline
		- only fine-tuned for text based tasks
	- Image + Seq2Seq
		- ResNeXt Pretrained on 3.5B IG Images

### Reverse Engineering Configurations of Neural Text Generation Models

- Models leave detectable artifacts
	- Do some modeling choices leave behind more artifacts than others?
	- Can we distinguish between text generation models based on text generated alone?
	- Which model configurations leave behind the most detectable artifacts?
- Given generated text, we try to predict the configurations.
- Configurations
	- Top-K and Top-p nucleus sampling
	- Length of initial conditional text
	- Model size (base, large, mega)


### Span-ConverRT: Few-shot Span Extraction for Dialog with Pretrained Conversational Representations

- Summary
	- light-weight model for dialog slot-filling
	- also present a unique dataset
- Multi-Woz has categorical slots and ATIS has single turn interactions
- Restaurants8k
	- GitHub available
	- 5 slots (time, date, people, first, last name)
- Models
	- Subword vectors + CNN + CRF
	- Read about ConverRT