---
layout: post
mathjax: true
title: "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension"
tags:
- NLG
categories:
- Applied
thumbnail_path: blog/personal/chatbot.png
---

[Paper Link](https://arxiv.org/pdf/1910.13461.pdf)

### Introduction

- BART: a denoising autoencoder for pretraining sequence-to-sequence models.
- BART is trained by
	1. corrupting text with an arbitrary noising function, and
	2. learning a model to reconstruct the original text using a Seq2Seq.
- Can be seen as generalizing BERT (encoder) and GPT (decoder).


### Architecture

It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a
left-to-right autoregressive decoder.

{% include figure.html path="blog/personal/bart_bert_gpt.png" alt="BART vs BERT vs GPT" %}

**Difference to BERT**

- Each layer of the decoder additionally performs cross-attention over the final hidden layer of the encoder (as in the transformer sequence-to-sequence model).
- BERT uses an additional feed-forward network before word prediction, which BART does not.

**Pre-training BART**

{% include figure.html path="blog/personal/bart_transformations.png" alt="BART Transformations" %}

**Fine-tuning BART**

1. Sequence Classification - Same input is fed to both encoder and decoder.
2. Token Classification - same as sequence classification but token level.
3. Sequence Generation - standard encoder decoder.
4. Machine Translation - TBR.

### Results

{% include figure.html path="blog/personal/bart_results.png" alt="BART Results" %}

