---
layout: post
mathjax: true
title: "Single Headed Attention RNN"
tags:
- Core
categories:
- Research
thumbnail_path: blog/personal/deepl.png
---

[Paper Link](https://arxiv.org/pdf/1911.11423.pdf)

### Overview

1. The goal is to build a simple language model that can run in a single GPU and still do well.
2. Due to above, the goal is to avoid Transformer architectures and see if we can use the traditional LSTM.

### Architecture

{% include figure.html path="blog/personal/sha-rnn.png" alt="SHA-RNN" %}

- The model consists of trainable embedding later, a single headed attention LSTM layer followed by a dense softmax layer.
- The weights from trainable embedding layer and the dense softmax layers are shared.

**Single Headed Attention**
- Similar to transformers's MHA but just 1 head :P

**Boom Layer**
- 2 feed-forward layers with GELU (project big and then back). 


### Results

| Model | Bits Per Char |
|-------|-----|
| LSTM | 1.182 |
| SHA-RNN |  1.100 |
| Adaptive Transformer | 1.04 |
