---
layout: post
mathjax: true
title: "Natural Language Processing"
tags:
- NLP
categories:
- Work
thumbnail_path: blog/personal/nlp.png
---

**Core Concepts**

**Word Vectors**

**Beyond Word Vectors**

1. [Deep Averaging Networks](https://www.weak-learner.com/blog/2019/07/31/deep-averaging-networks/)
2. [BERT](https://www.weak-learner.com/blog/2019/08/16/bert/)

**Classification**

1. [Hierarchical Attention Networks](https://www.weak-learner.com/blog/2019/06/23/hierarchical_attention_networks/)

**Named Entity Recognition & Slot Filling**

1. [SUTIME: A Library for Recognizing and Normalizing Time Expressions](https://www.weak-learner.com/blog/2019/06/20/SUTime/)

**Conversational AI**

1. [Dialog State Tracking: A Neural Reading Comprehension Approach](https://www.weak-learner.com/blog/2019/08/19/dst-mrc/)
2. [Scalable Multi-Domain Dialogue State Tracking](https://www.weak-learner.com/blog/2019/08/20/scalable-multidomain-dst/)
3. [Dialog Context Language Modeling with Recurrent Neural Networks](https://www.weak-learner.com/blog/2019/11/01/dialog-context-language-modeling/)

**Machine Reading Comprehension**

1. [BiDirectional Attention Flow](https://www.weak-learner.com/blog/2019/08/13/bidirectional-attention-flow/)
2. [Dynamic Co-attention Networks](https://www.weak-learner.com/blog/2019/08/13/dynamic-coattention-network/)

**Machine Translation**

1. [Attention Is All You Need](https://www.weak-learner.com/blog/2019/08/01/attention-is-all-you-need/)
2. [Blockwise Parallel Decoding for Deep Autoregressive Models](https://www.weak-learner.com/blog/2019/08/08/blockwise-parallel-decoding-for-deep-autoregressive-models/)