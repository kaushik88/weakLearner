### Introduction

- We develop a dataefficient Graph Convolutional Network (GCN) algorithm PinSage, which combines efficient random walks and graph convolutions to generate embeddings
of nodes (i.e., items) that incorporate both graph structure as well as node feature information.
- The core idea behind GCNs is to learn how to iteratively aggregate feature information from local graph neighborhoods using neural networks.
-  Here a single “convolution” operation transforms and aggregates feature information from a node’s one-hop graph neighborhood, and 
by stacking multiple such convolutions information can be propagated across far reaches of a graph.
- Unlike purely content-based deep models (e.g., recurrent neural networks [3]), GCNs leverage both content information as well as graph structure.
- All existing GCN-based recommender systems require operating on the full graph Laplacian during training

**Contributions**

- On-the-fly convolutions: PinSage algorithm performs efficient, localized convolutions by sampling the neighborhood around a node and dynamically constructing a
computation graph from this sampled neighborhood. 
- Producer-consumer minibatch construction: We develop a producer-consumer architecture for constructing minibatches that ensures maximal GPU utilization during model training.
- Efficient MapReduce inference: Given a fully-trained GCN model, we design an efficient MapReduce pipeline that can distribute the trained model to generate embeddings for billions of
nodes, while minimizing repeated computations.
- Constructing convolutions via random walks: Taking full neighborhoods of nodes to perform convolutions (Fig. 1) would result in huge computation graphs, so we resort to sampling
- Importance pooling: We introduce a method to weigh the importance of node features in this aggregation based upon randomwalk similarity measures, leading to a 46% performance gain in
offline evaluation metrics.
- Curriculum training: We design a curriculum training scheme, where the algorithm is fed harder-and-harder examples during training, resulting in a 12% performance gain.

### Model

- We model the Pinterest environment as a bipartite graph consisting of nodes in two disjoint sets, I (containing pins) and C (containing boards). 
Note, however, that our approach is also naturally generalizable, with I being viewed as a set of items and C as a set of user-defined contexts or collections.
- These embeddings are then used for recommender system candidate generation via nearest neighbor lookup (i.e., given a pin, find related pins) or as features in machine learning systems for
ranking the candidates.
- For notational convenience and generality, when we describe the PinSage algorithm, we simply refer to the node set of the full graph with V = I ∪ C.

**Convolve operator**

- Fixed representation of neighborhood - weighted sum or element wise mean of (Dense + ReLU)
- New representation - concat and apply non-linearity
- Normalize

**Neighborhood Selection/ Importance Pooling**

- Whereas previous GCN approaches simply examine k-hop graph neighborhoods, in PinSage we define importance-based neighborhoods, where the neighborhood of a nodeu is defined as theT nodes
that exert the most influence on node u.
- Concretely, we simulate random walks starting from node u and compute the L1-normalized visit count of nodes visited by the random walk.
- The neighborhood of u is then defined as the top T nodes with the highest normalized visit counts with respect to node u.
- In particular, we implement γ in Algorithm 1 as a weighted-mean, with weights defined according to the L1 normalized visit counts.

**Stacking Convolutions**

- Note that the model parameters in Algorithm 1 (Q, q, W, and w) are shared across the nodes but differ between layers.
- We first compute the neighborhoods of each node and then apply K convolutional iterations to generate the layer-K representations of the target nodes.
- The output of the final convolutional layer is then fed through a fullyconnected neural network to generate the final output embeddings.

- The full set of parameters of our model which we then learn is: the weight and bias parameters for each convolutional layer 
(Q(k), q(k), W(k), w(k), ∀k ∈ {1, ...,K}) as well as the parameters of the final dense neural network layer, G1, G2, and g.

**Training Details**

- Max-margin ranking loss

**Hard Negatives**
- For each positive training example (i.e., item pair (q,i)), we add “hard” negative examples, i.e., items that are somewhat related to the query item q, but not as related
as the positive item i.
- They are generated by ranking items in a graph according to their Personalized PageRank scores with respect to query item q.
- Read C. Pixie: A System for Recommending 3+ Billion Items to 200+ Million Users in Real-Time. WWW (2018).
- Items ranked at 2000-5000 are randomly sampled as hard negative items.

**Node Embeddings via Mapreduce**
- Naively computing embeddings for nodes using Algorithm 2 leads to repeated computations caused by the overlap between K-hop neighborhoods of nodes.
- One MapReduce job is used to project all pins to a lowdimensional latent space, where the aggregation operation
will be performed (Algorithm 1, Line 1).
- Another MapReduce job is then used to join the resulting pin representations with the ids of the boards they occur in, and the board embedding is computed by pooling the features of
its (sampled) neighbors.

### Results
